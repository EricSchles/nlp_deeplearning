{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformations\n",
    "\n",
    "This tutorial features:\n",
    "* tokenization\n",
    "* stemming\n",
    "* lemmatization\n",
    "* tf-idf\n",
    "* encoders\n",
    "* decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy tokens:\n",
      "[\n",
      ", Hello, there, ,, Karl, Marx, ;, how, are, you, doing, today, ?, Was, the, play, good, and, fun, ?, \n",
      ", Did, you, laugh, at, the, jokes, ,, filling, your, eyes, with, glee]\n",
      "nltk tokens:\n",
      "['Hello', 'there', ',', 'Karl', 'Marx', ';', 'how', 'are', 'you', 'doing', 'today', '?', 'Was', 'the', 'play', 'good', 'and', 'fun', '?', 'Did', 'you', 'laugh', 'at', 'the', 'jokes', ',', 'filling', 'your', 'eyes', 'with', 'glee']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"\"\"\n",
    "Hello there, Karl Marx; how are you doing today? Was the play good and fun? \n",
    "Did you laugh at the jokes, filling your eyes with glee\"\"\"\n",
    "\n",
    "#In Spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(sentence)\n",
    "print(\"spacy tokens:\")\n",
    "tokens = list(doc)\n",
    "print(tokens)\n",
    "\n",
    "#In NLTK\n",
    "print(\"nltk tokens:\")\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Customization\n",
    "\n",
    "If you want to customize Spacy's tokenizer it's pretty straight forward:\n",
    "\n",
    "Just check out:\n",
    "\n",
    "* https://spacy.io/docs/usage/customizing-tokenizer\n",
    "* https://spacy.io/docs/api/tokenizer (api reference)\n",
    "\n",
    "How to do customization in NLTK:\n",
    "\n",
    "* http://stackoverflow.com/questions/3930267/nltk-custom-tokenizer-and-tagger\n",
    "* http://stackoverflow.com/questions/14095971/how-to-tweak-the-nltk-sentence-tokenizer\n",
    "* http://www.nltk.org/api/nltk.tokenize.html (api reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('walking')\n",
    "#Note: Spacy doesn't have a stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs -> dog\n",
      "ponies -> pony\n",
      "Sentence parsed by NLTK\n",
      "This isn't a dogs and ponies show, after all!  We are barely making any molla...\n",
      "Sentence parsed by Spacy\n",
      "this be not a dog and pony show , after all !   -PRON- be barely make any molla ...\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "#reference: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "sentence = \"This isn't a dogs and ponies show, after all!  We are barely making any molla...\" \n",
    "#NLTK\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(\"dogs ->\",wnl.lemmatize('dogs'))\n",
    "print(\"ponies ->\",wnl.lemmatize('ponies'))\n",
    "print(\"Sentence parsed by NLTK\")\n",
    "print(wnl.lemmatize(sentence))\n",
    "\n",
    "#Spacy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "parsedData = parser(sentence)\n",
    "print(\"Sentence parsed by Spacy\")\n",
    "print(\" \".join([token.lemma_ for token in parsedData]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining Stemming and Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "wnl = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(wnl.lemmatize('walking'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 98)\t0.481029274755\n",
      "  (0, 50)\t0.619923719835\n",
      "  (0, 38)\t0.619923719835\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency - Inverse Document Frequency\n",
    "\n",
    "#Reference One: http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "#Reference Two: http://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate\n",
    "#NLTK\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "document = \"\"\"\n",
    "Trump loses some sway with fellow Republicans\n",
    "In the aftermath of President Trump's firing of FBI Director James Comey, two things have happened when it comes to Trump's standing with his own party. One, congressional Republicans haven't abandoned Trump on Comey (yes, some have criticized the timing, but there isn't a growing GOP demand for a special counsel). Two, enough of them are beginning to buck their president on non-Comey items. That includes Sens. John McCain (R-AZ) and Ben Sasse (R-NE) saying they're opposed to Trump's pick to be U.S. trade representative, and three Republicans who worked to defeat a GOP-backed resolution to repeal an Obama environmental regulation.\n",
    "\n",
    "So while Republicans aren't in open revolt when it comes to Comey directly, we've seen some signs in the last 24 hours how Trump's legislative agenda, as well as his staffing priorities, could be in peril. And as for Democrats, it's hard to imagine how the president has the juice to win over any members of the opposition. Don't forget: At this same point in time of George W. Bush's presidency, there were a handful of Democrats who supported his tax cuts, and Ted Kennedy was working with his administration on education.\n",
    "\n",
    "NBC's Lester Holt interviews Trump\n",
    "NBC's Lester Holt today interviews President Trump, and it will air on \"Nightly News.\" What will President Trump say about his recent firing of FBI Director James Comey? Be sure to watch!\n",
    "\"\"\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def stem_lemmatize_tokens(tokens, stemmer):\n",
    "    stem_lemmatized = []\n",
    "    for item in tokens:\n",
    "        stem_lemmatized.append(wnl.lemmatize(item))\n",
    "    return stem_lemmatized\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_lemmatize_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "\n",
    "lowers = document.lower()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "no_punctuation = lowers.translate(translator)\n",
    "processed_text = tokenize(no_punctuation)\n",
    "#this can take some time\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfs = tfidf.fit_transform(processed_text)\n",
    "for index,elem in enumerate(processed_text):\n",
    "    print(elem,tfs[index])\n",
    "snippet = \"NBC's Lester Holt interviews Trump\"\n",
    "response = tfidf.transform([snippet])\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder - Decoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "# Encoding\n",
    "# encode class values as integers\n",
    "def one_hot_encoding(df):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df)\n",
    "    encoded_df = encoder.transform(df)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    return np_utils.to_categorical(encoded_df)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['e'] = [random.choice(('Chicago', 'Boston', 'New York',None)) for i in range(df.shape[0])]\n",
    "df = df[pd.notnull(df[\"e\"])]\n",
    "encoded = one_hot_encoding(df[\"e\"])\n",
    "\n",
    "# Decoding\n",
    "\n",
    "#http://stackoverflow.com/questions/22548731/how-to-reverse-sklearn-onehotencoder-transform-to-recover-original-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
