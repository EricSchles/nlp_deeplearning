{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Processing\n",
    "\n",
    "This tutorial features:\n",
    "* part of speech tagging\n",
    "* named entity recognition\n",
    "    * address processing\n",
    "    * phone number processing\n",
    "* text classification\n",
    "* emotional valence / sentiment analysis\n",
    "* word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence parsed by Spacy\n",
      "This DET\n",
      "is VERB\n",
      "n't ADV\n",
      "a DET\n",
      "dogs NOUN\n",
      "and CCONJ\n",
      "ponies NOUN\n",
      "show VERB\n",
      ", PUNCT\n",
      "after ADV\n",
      "all ADV\n",
      "! PUNCT\n",
      "  SPACE\n",
      "We PRON\n",
      "are VERB\n",
      "barely ADV\n",
      "making VERB\n",
      "any DET\n",
      "molla NOUN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "\n",
    "#Reference: https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "sentence = \"This isn't a dogs and ponies show, after all!  We are barely making any molla...\" \n",
    "\n",
    "#Spacy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "parsedData = parser(sentence)\n",
    "print(\"Sentence parsed by Spacy\")\n",
    "# Let's look at the part of speech tags of the first sentence\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "-------------- entities only ---------------\n",
      "380 ORG Apple\n",
      "377 PERSON Steve Jobs\n",
      "387 DATE October\n"
     ]
    }
   ],
   "source": [
    "#reference: https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "# Let's look at the named entities of this example:\n",
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Address and phone number parsing:\n",
    "\n",
    "# https://github.com/EricSchles/investigator/blob/master/app/text_parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greeting'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Classification\n",
    "\n",
    "# Reference: https://github.com/EricSchles/csvconf_talk/blob/master/scraping_craigslist/app/text_classify.py\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "def train_classifier(text,labels):\n",
    "    #http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#parameter-tuning-using-grid-search\n",
    "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf__alpha': (1e-2, 1e-3),}\n",
    "    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer())),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    return gs_clf.fit(text,labels)\n",
    "        \n",
    "def classify_text(classifier,input_data):\n",
    "    return classifier.predict([input_data])[0]\n",
    "\n",
    "text = [\"Hello\",\"Hi\",\"How are you?\",\"How's it going\",\"What's up?\",\"How have you been?\",\"hi!\",\"Hi how are you?\",\n",
    "        \"Bye\",\"Goodbye\",\"see ya\",\"See ya!\",\"See you later!\",\"See you again\",\"Have fun!\",\"later\",\"Later\",\"Later!\"]\n",
    "greeting = \"greeting,\"*8\n",
    "goodbye = \"goodbye,\"*10\n",
    "labels = greeting.split(\",\")[:-1] + goodbye.split(\",\")[:-1] \n",
    "clf = train_classifier(text,labels)\n",
    "classify_text(clf,\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.----------------------------- {'pos': 0.746, 'neg': 0.0, 'neu': 0.254, 'compound': 0.8316}\n",
      "VADER is not smart, handsome, nor funny.------------------------- {'pos': 0.0, 'neg': 0.646, 'neu': 0.354, 'compound': -0.7424}\n",
      "VADER is smart, handsome, and funny!----------------------------- {'pos': 0.752, 'neg': 0.0, 'neu': 0.248, 'compound': 0.8439}\n",
      "VADER is very smart, handsome, and funny.------------------------ {'pos': 0.701, 'neg': 0.0, 'neu': 0.299, 'compound': 0.8545}\n",
      "VADER is VERY SMART, handsome, and FUNNY.------------------------ {'pos': 0.754, 'neg': 0.0, 'neu': 0.246, 'compound': 0.9227}\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!---------------------- {'pos': 0.767, 'neg': 0.0, 'neu': 0.233, 'compound': 0.9342}\n",
      "VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!--------- {'pos': 0.706, 'neg': 0.0, 'neu': 0.294, 'compound': 0.9469}\n",
      "The book was good.----------------------------------------------- {'pos': 0.492, 'neg': 0.0, 'neu': 0.508, 'compound': 0.4404}\n",
      "The book was kind of good.--------------------------------------- {'pos': 0.343, 'neg': 0.0, 'neu': 0.657, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'pos': 0.094, 'neg': 0.327, 'neu': 0.579, 'compound': -0.7042}\n",
      "At least it isn't a horrible book.------------------------------- {'pos': 0.363, 'neg': 0.0, 'neu': 0.637, 'compound': 0.431}\n",
      "Make sure you :) or :D today!------------------------------------ {'pos': 0.706, 'neg': 0.0, 'neu': 0.294, 'compound': 0.8633}\n",
      "Today SUX!------------------------------------------------------- {'pos': 0.0, 'neg': 0.779, 'neu': 0.221, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol----------------------- {'pos': 0.251, 'neg': 0.179, 'neu': 0.569, 'compound': 0.2228}\n"
     ]
    }
   ],
   "source": [
    "# Emotional Valence\n",
    "\n",
    "#Also see: http://www.nltk.org/howto/sentiment.html (for nltk example)\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#note: depending on how you installed (e.g., using source code download versus pip install), you may need to import like this:\n",
    "#from vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- examples -------\n",
    "sentences = [\"VADER is smart, handsome, and funny.\",      # positive sentence example\n",
    "            \"VADER is not smart, handsome, nor funny.\",   # negation sentence example\n",
    "            \"VADER is smart, handsome, and funny!\",       # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "            \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "            \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "            \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "            \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "            \"The book was good.\",                                     # positive sentence\n",
    "            \"The book was kind of good.\",                 # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "            \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "            \"At least it isn't a horrible book.\",         # negated negative sentence with contraction\n",
    "            \"Make sure you :) or :D today!\",              # emoticons handled\n",
    "            \"Today SUX!\",                                 # negative slang with capitalization emphasis\n",
    "            \"Today only kinda sux! But I'll get by, lol\"  # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             ]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word Embeddings\n",
    "\n",
    "# Explanation w/ pictures http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "# How to implement: http://deeplearning.net/tutorial/rnnslu.html\n",
    "# worked example: http://vene.ro/blog/word-movers-distance-in-python.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
